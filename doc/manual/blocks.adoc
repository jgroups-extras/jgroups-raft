
[[BuildingBlocks]]
== Building blocks

Similar to JGroups' building blocks, jgroups-raft also has building blocks, which provide additional functionality on
top of a `RaftHandle`. They are typically given a JChannel, create a `RaftHandle` and register themselves as
`StateMachine` with the handle. Building blocks offer a different interface to the users, e.g. a replicated hashmap
with puts and gets, or a distributed counter or lock.



[[ReplicatedStateMachine]]
=== ReplicatedStateMachine

`ReplicatedStateMachine` is a key-value store replicating its contents to all cluster members. Contrary to the JGroups
equivalent (`ReplicatedHashMap`), changes are replicated by consensus and logged to a persistent log.

While the JGroups version is allowed to make progress during network partitions, and users need to merge possibly
diverging state from different partitions after a partition heals, `ReplicatedStateMachine` will allow progress only in
the _majority partition_, so no state merging needs to be done after a partition heals.

Not having to merge state is certainly simpler, but comes at the expense of availability: if `N/2+1` members leave or
split into different partitions, `ReplicatedStateMachine` will be unavailable (all requests will time out).

However, the advantage is that the members' states will never diverge.

`ReplicatedStateMachine` requires a `JChannel` in its constructor and has `put()`, `get()` and `remove()` methods.
The code below shows how to create an instance of `ReplicatedStateMachine` and add an element to it:

[source,java]
----
protected void start(String raft_id) throws Exception {
    JChannel ch=new JChannel("raft.xml").name(raft_id);
    ReplicatedStateMachine<String,String> rsm=new ReplicatedStateMachine<>(ch)
        .raftId(raft_id);
    ch.connect("rsm-cluster");
    rsm.put("name", "Bela");
}
----

There's a demo `ReplicatedStateMachineDemo` which can be used to interactively use `ReplicatedStateMachine`.

==== Reads and consensus

`ReplicatedStateMachine` has a configurable behavior for reads, using quorum reads by default. This means that all read
requests are sent through `RAFT` and require a majority to complete. This provides a linearizable read, but, on the other
hand, it takes longer to complete.

If an application does not require linearizable reads, it can change the behavior and only read the value locally,
possibly stale, but having a faster response since the request does not go through `RAFT`.

The behavior is changed by calling `ReplicatedStateMachine.allowDirtyReads(boolean)`.

NOTE: A quorum read creates an entry in the persistent log. See
https://github.com/belaban/jgroups-raft/issues/18[Issue 18] for details.

[[CounterService]]
=== CounterService

`CounterService` provides a replicated counter which can be used in a synchronous (`SyncCounter`) and asynchronous
(`AsyncCounter`) mode. The base interface is `Counter`:

[source,java]
----
public interface Counter {
    String                getName();
    SyncCounter           sync();
    AsyncCounter          async();
    <T extends Counter> T withOptions(Options opts);
}
----

The `sync()` and `async()` methods can be used to switch between the modes. The `withOptions()` method can be used to
make a counter ignore return values (see below).



==== Synchronous counters
Synchronous counters block until an operation has completed, and return a value. The interface `SyncCounter` is:

[source,java]
----
public interface SyncCounter extends Counter {
    long            get();
    long            getLocal();
    void            set(long new_value);
    default boolean compareAndSet(long exp, long upd) {return compareAndSwap(exp, upd) == exp;}
    long            compareAndSwap(long expect, long update);
    default long    incrementAndGet() {return addAndGet(1L);}
    default long    decrementAndGet() {return addAndGet(-1L);}
    long            addAndGet(long delta);
}
----

The methods are:

[%autowidth]
|====
| Name | Description

| `get` | Returns the value of the counter. If the CounterService allows for dirty reads (see below), `getLocal()` is
          called. Otherwise, a remote get request is sent to the leader.
| `getLocal` | Returns the value of the counter. This call is purely local and may return a stale value.
| `set` | Sets the value of the counter
| `compareAndSet` | Atomically updates the counter using a CAS operation
| `compareAndSwap` | Atomically updates the counter using a compare-and-swap operation
| `incrementAndGet` | Atomically increments the counter and returns the new value
| `decrementAndGet` | Atomically decrements the counter and returns the new value
| `addAndGet` | Atomically adds the given value to the current value
|====

The synchronous methods block until consensus has been reached. This means that a method (e.g. `addAndGet()`) may block
indefinitely, e.g. when less than a majority of members have ack'ed the change.



==== Asynchronous counters

Asynchronous counters are defined in `AsyncCounter`:

[source,java]
----
public interface AsyncCounter extends Counter {
    default CompletionStage<Long>    get() {return addAndGet(0);}
    CompletionStage<Long>            getLocal();
    CompletionStage<Void>            set(long new_value);

    default CompletionStage<Boolean> compareAndSet(long expect, long update) {
        return compareAndSwap(expect, update).thenApply(value -> value == expect);
    }
    CompletionStage<Long>            compareAndSwap(long expect, long update);
    default CompletionStage<Long> incrementAndGet() {
        return addAndGet(1);
    }
    default CompletionStage<Long>    decrementAndGet() {
        return addAndGet(-1);
    }
    CompletionStage<Long>            addAndGet(long delta);
}
----

The semantics of the methods correspond to their synchronous counterparts, but they return a `CompletionStage` instead
of the value. An example is shown below:

[source,java]
----
public void testCompareAndSwapChained() {
    AsyncCounter counter=counter_service.getOrCreateCounte("ctr", 0).async();
    final long initialValue = 100;
    final long finalValue = 10;

    counter.set(initialValue).toCompletableFuture().join();

    AtomicLong rv = new AtomicLong();
    boolean result = counter.compareAndSwap(1, finalValue)
            .thenCompose(rValue -> {
                rv.set(rValue);
                return counters.get(0).compareAndSwap(rValue, finalValue);
            })
            .thenApply(value -> value == initialValue)
            .toCompletableFuture()
            .join();

    assert result;
    assert initialValue == rv.longValue();
}
----


A `Counter` implementation is created via the `CounterService` building block:

[source,java]
----
public class CounterService implements StateMachine {
    public CounterService(Channel ch);
    public long           replTimeout();
    public CounterService replTimeout(long timeout);
    public boolean        allowDirtyReads();
    public CounterService allowDirtyReads(boolean flag);
    public CounterService raftId(String id);

    /**
     * Returns an existing counter, or creates a new one if none exists
     * @param name Name of the counter, different counters have to have different names
     * @param initial_value The initial value of a new counter if there is no existing counter.
     * Ignored if the counter already exists
     * @return The counter implementation
     */
    public Counter getOrCreateCounter(String name, long initial_value) throws Exception;


    /**
     * Deletes a counter instance (on the coordinator)
     * @param name The name of the counter. No-op if the counter doesn't exist
     */
    public void deleteCounter(String name) throws Exception;
}
----

`CounterService` is mainly used to get an existing or create a new `Counter` implementation (`getOrCreateCounter()`), or
to delete an existing counter (`deleteCounter()`).

To create an instance of `CounterService`, a JChannel has to be passed to the constructor. The sample code below
shows how to use this:

[source,java]
----
protected void start(String raft_id) throws Exception {
    JChannel ch=new JChannel("raft.xml").name(raft_id);
    CounterService cs=new CounterService(ch);                   // <1>
    ch.connect("counter-cluster");
    SyncCounter counter=cs.getOrCreateCounter("mycounter", 1);  // <2>
    counter.incrementAndGet();                                  // <3>
    counter.compareAndSet(2, 5);                                // <4>
    long current_value=counter.get();                           // <5>
}
----
<1> First a `CounterService` is created and given a reference to a channel
<2> Once the member has joined the cluster, we create a counter named "mycounter" with an initial value of 1
<3> The counter is then incremented to 2
<4> Now a compare-and-set operation sets the counter to 5 if it was 2
<5> The last operation fetches the current value of "mycounter"


Any member in the cluster can change the same counter and all operations are ordered by the Raft leader, which causes
the replicated counters to have exactly the same value in all members.

Comparing this to the JGroups equivalent, a jgroups-raft counter never diverges in different members, again at the
expense of availability. In the JGroups version, counters are always available, but may diverge, e.g. in a split brain
scenario, and have to be reconciled by the application after the split brain is resolved.

There's a demo `CounterServiceDemo` which can be used to interactively manipulate replicated counters.


==== Reads and consensus

Currently (as of jgroups-raft version 0.4), reading a counter is by default _dirty_, meaning that a read may return a
stale value.

This can be changed by calling `counter_service.allowDirtyReads(false)`.

However, this inserts a dummy _read log entry_ which returns the value of counter when committed. Since this dummy entry
is ordered correctly wrt writes in the log, it will always return correct values.

The cost is that reads take up space in the persistent logs and that we need consensus (majority) for reads. In the next
release of jgroups-raft, the mechanism for client reads as suggested in the Raft paper will be implemented. See
https://github.com/belaban/jgroups-raft/issues/18[Issue 18] for details.

==== Ignoring return values
Sometimes, a caller is not interested in the result of an operation. E.g. a stress test may want to update a counter
many times, e.g. with many different threads, and only then fetch the final counter value. When this is the case,
an _option_ can be used with a counter:

[source,java]
----
public void testIgnoreReturnValue() {
    SyncCounter counter=counter_service.getOrCreateCounter("ctr", 0);
    long ret=counter.incrementAndGet(); // <1>

    counter=counter.withOptions(Options.create(true)); // <2>
    ret=counter.incrementAndGet(); // <3>
    assert ret == 0;
    ret=counter.getLocal(); // <4>
}
----

In (1), a counter is incremented and the new value returned. This returns `1`

In (2) , a counter is created with an `Option`, which declares that return values are to be ignored. Consequently,
when we increment the counter in (3), the return value is `0`, although the counter was indeed incremented, as
shown when fetching the value in (4).

Returning `0` may not be the most clever use of options, but is the result of autoboxing a `null` `Long` value into
a `long`. The idea is that the result of an operation that has this option set, should not be assigned to a variable.

When the ignore-return-value option is set, `REDIRECT` doesn't need to serialize and send the result from the leader
to the follower, and `RAFT` does not need to serialize the result into a `byte[]` array, either. The cost reduction
here may not be insignificant, depending on the (serialized) size of the result values and the frequency of operations.


=== Cluster singleton service

A _singleton service_ is a service which is supposed to run only once in an entire cluster. Typically, in JGroups, a
singleton service is started on the first member of a cluster. For example, if we have `{A,B,C,D,E}`, the singleton
service (or services) would be running on `A`.

If we have a partition, such that the cluster falls apart into `{A,B,C}` and `{D,E}`, then an _additional_ singleton
would be started on `D`, as `D` became coordinator and doesn't know `{A,B,C}` didn't leave, but were partitioned away
instead.

When the partition ends, if `D` is not coordinator anymore, it would stop its singleton services.

If multiple singletons (as provided by JGroups, e.g. during a network split) cannot be tolerated by the application,
and the application has a requirement that _at most one singleton service_ can be running (better none than two),
jgroups-raft can be used.

The mechanism to implement singleton services in jgroups-raft is leader election: it is guaranteed that at most one
leader exists in a given cluster at the same time. This is exactly what we need for singletons. The code below shows
how to do this:

[source,java]
----
JChannel ch=null;
RaftHandle handle=new RaftHandle(ch, this); // <1>
handle.addRoleListener(role -> {            // <2>
    if(role == Role.Leader)                 // <3>
        // start singleton services
    else
        // stop singleton services
});
----
<1> A `RaftHandle` is created over a channel
<2> A `RAFT.RoleChange` callback is registered with the handle. Alternatively, `addRoleListener()` could be called
    directly on an instance of `RAFT` retrieved from the protocol stack associated with the given channel
<3> When we become the Raft leader, the singleton services can be started, when not, they should be stopped (if running)

In jgroups-raft, utilizing JGroups' views and an altered leader election algorithm strengthens the singleton more than
the one described by Raft. In a usual Raft implementation, the members compete to become leaders based on time outs.
This behavior causes disruptions in the cluster, and it allows for multiple leaders at the same time but on different terms.

More details about the jgroups-raft custom election algorithm are available in the design documents. In a few words,
our implementation is more robust and less prone to disruption of competing members. These design changes converge in
a more stable singleton service.

[CAUTION]
.Singleton is **not** a distributed lock.
====
Utilizing the singleton service as a distributed lock to access critical sections or resources is dangerous during
network partitions. The current leader may split in a minority partition, which means the majority elects another node.
If this happens while a node is in a critical section, it could result in two nodes accessing the resource.
====



