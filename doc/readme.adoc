= Raft in JGroups

== Goals

. Implement the RAFT consensus protocol in JGroups
. Provide an API to RAFT
. Implement etcd (?) [API and REST interface]
. ZooKeeper impl using etcd impl (?)
. View installation via consensus ?


== Advantages of using JGroups as basis for a RAFT impl

* Transports already available: UDP, TCP
** Contains thread pools, priority delivery (OOB), batching etc
* Variety of discovery protocols
* Encryption, authentication, compression
* Fragmentation, reliability over UDP
* Multicasting for larger clusters
* Failure detection
* Sync/async cluster RPC


== Design overview

* Raft building block over a channel
** Communicates with RAFT protocol via events (like COUNTER or CENTRAL_LOCK)


== Design issues

* Separate protocols for
** Leader election
*** Could be omitted as JGroups already has a leader
**** No, we cannot use this (see below under Issues) !
**** But we may be able to reuse the JGroups failure detection protocols for the heartbeat
*** Majority partition makes progress
** Log replication
** Log safety
** Client interaction
* These protocols would communicate via events
* Heartbeat is not needed for failure detection, but only for log replication
  and log safety
* Send heartbeat messages as NO_RELIABILITY ? -> Do we care about JGroups failure detection and view management ?
** Or could we use JGroups' failure detection ?
*** Don't use RAFT's heartbeat mechanism
*** Start an election when there's a view change which doesn't contain the current leader
*** JGroups coordinator != RAFT leader


== Replace RAFT's heartbeat mechanism

* The problem is that the AppendEntries RPC is used for log shipping *and* heartbeating
* The heartbeating part is done (ca. every 20-30 ms) even when no logs have to be shipped
** This is annoying as
*** This causes unneeded traffic to all cluster nodes
*** JGroups already does this (duplication of functionality)
* Suggestion: drop the heartbeating (not the log replication) part of AppendEntries and replace it with the JGroups equivalent
* The RAFT heartbeat is used to do the following:
. Keep the followers from becoming candidates and start their own elections
. Send the current term to the followers
. Send the identity of the leader to the followers
* This could be replaced with the following:
. JGroups failure detection. An election is only started when the current leader is removed from the view. Note that
  JGroups coordinator != RAFT leader
. Current term: this can be done with a multicast to all followers, or by state transfer when a new follower starts. This state transfer
  is done anyway for new followers (InstallSnapshot)
. Identity of the leader: same as above (multicast and/or sent as part of the state to a new follower)

* The advantage of this would be that we
** Separate heartbeating from log replication (RAFT does both with the `AppendEntries` RPC)
** Eliminate constant traffic caused by heartbeating and
** Remove redundant functionality of RAFT that's already part of JGroups. In addition, JGroups provides a number of
   (customizable) failure detection protocols.
* Issues: look at whether merging can be done with this mechanism, too


== Misc

* Seperate GitHub project for now for RAFT protocol and building block
** May be moved into JGroups once it is stable
** But for now, with a separate project we can release quickly and independently
* Separate project for etcd consuming RAFT and JGroups ?
* Mailing list on google
** Potential contributors from Lyon and Newcastle uni (MarkL)
*** Julien Ponge: julien.ponge@insa-lyon.fr
* Use of LevelDB / JDBM2 for persistence ?


== Issues

* What happens with client requests when no leader is elected ?
** Are they queued ?

* Do clients block until consensus has been reached, before they get the result ?
** For a get() this makes sense, but for a write ?

* Log replication message: sent to all, or only to those which have missing log entries ?
** Probably to all, as this also serves as heartbeat
*** Not very efficient to send *all* missing log entries to *all* members !

* We cannot use JGroups leader election (coordinators) because *a new leader may not contain
  all of the committed log entries !*
  ** In RAFT's leader election algorithm, only candidates with all (or the most) committed entries can become leaders









